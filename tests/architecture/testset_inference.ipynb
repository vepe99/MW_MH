{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 10:31:08.905301: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-13 10:31:08.953264: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-13 10:31:10.483762: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import optuna\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer perceptron (MLP) to be obtain parameters of Coupling layers\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input : int \n",
    "        Number of input neurons, depend on the dimensions of the input data. \n",
    "    n_output : int \n",
    "        Number of output neurons, depend on the number of parameters needed for the Coupling layers.\n",
    "    n_hidden : int\n",
    "        Number of hidden neurons in each layer.\n",
    "    n_layers : int\n",
    "        Number of layers in the network.\n",
    "    neg_slope : float\n",
    "        Negative slope for the leaky ReLU activation function.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_input, n_output, n_hidden, n_layers=4, neg_slope=0.2) -> None:\n",
    "        super().__init__()\n",
    "        ins = torch.ones(n_layers)*n_hidden\n",
    "        ins[0] = n_input\n",
    "        outs = torch.ones(n_layers)*n_hidden\n",
    "        outs[-1] = n_output\n",
    "        Lin_layers = list(map(nn.Linear, ins.type(torch.int), outs.type(torch.int)))\n",
    "        ReLu_layers = [nn.LeakyReLU(neg_slope) for _ in range(n_layers)]\n",
    "        self.network = nn.Sequential(*itertools.chain(*zip(Lin_layers, ReLu_layers)))\n",
    "        # self.network.apply(init_weights)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x.float())\n",
    "\n",
    "    \n",
    "class GLOW_conv(nn.Module):\n",
    "    def __init__(self, n_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.n_dim = n_dim\n",
    "\n",
    "        #Initialize W as orthogonal matrix and decompose it into P, L, U, the learned parameters\n",
    "        W_initialize = nn.init.orthogonal_(torch.randn(self.n_dim, self.n_dim))\n",
    "        P, L_, U_ = torch.linalg.lu(W_initialize)\n",
    "\n",
    "        #P not changed (no grad) but it needs to be stored in the state_dict\n",
    "        self.register_buffer(\"P\", P)\n",
    "\n",
    "        # Declare as model parameters\n",
    "        #Diagonal of U sourced out to S\n",
    "        S_ = torch.diagonal(U_)\n",
    "        self.S = nn.Parameter(S_)\n",
    "        self.L = nn.Parameter(L_)\n",
    "        #Declare with diagonal 0s, without changing U_ and thus S_\n",
    "        self.U = nn.Parameter(torch.triu(U_, diagonal=1))\n",
    "\n",
    "    def _get_W_and_logdet(self):\n",
    "        #Make sure the pieces stay in correct shape as in GLOW\n",
    "        L = torch.tril(self.L, diagonal=-1) + torch.diag(torch.ones(self.n_dim).to(self.L.device))\n",
    "        U = torch.triu(self.U, diagonal=1)\n",
    "        S = torch.diag(self.S)\n",
    "        \n",
    "        W = self.P@L@(U+S)\n",
    "        logdetW = torch.sum(torch.log(torch.abs(self.S)))\n",
    "\n",
    "        return W, logdetW  \n",
    "    \n",
    "    # Pass condition as extra argument, that is not used in the convolution\n",
    "    #it stayes untouched, does not get permuted with values that\n",
    "    #will be transformed\n",
    "    def forward(self, x, x_condition):\n",
    "        W, logdetW = self._get_W_and_logdet()\n",
    "        y = x.float()@W\n",
    "        return y, logdetW\n",
    "    \n",
    "    def backward(self, y, x_condition):\n",
    "        W, logdetW_inv = self._get_W_and_logdet()\n",
    "        #Just a minus needed\n",
    "        logdetW_inv = -logdetW_inv\n",
    "        W_inv = torch.linalg.inv(W)\n",
    "        x = y.float()@W_inv\n",
    "        return x, logdetW_inv\n",
    "\n",
    "class AffineCoupling(nn.Module):\n",
    "    \"\"\"\n",
    "    Affine Coupling layer for conditional normalizing flows.\n",
    "\n",
    "    Args:\n",
    "        dim_notcond (int): Dimension of the input not conditioned part.\n",
    "        dim_cond (int): Dimension of the input conditioned part.\n",
    "        network (nn.Module): Network architecture to use for the affine coupling layer.\n",
    "        network_args (tuple): Additional arguments to pass to the network architecture.\n",
    "\n",
    "    Attributes:\n",
    "        dim_notcond (int): Dimension of the input not conditioned part.\n",
    "        dim_cond (int): Dimension of the input conditioned part.\n",
    "        net_notcond (nn.Module): Network for the not conditioned part.\n",
    "        net_cond (nn.Module): Network for the conditioned part.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_notcond, dim_cond, network=MLP, network_args=(16, 4, 0.2)):\n",
    "        super().__init__()\n",
    "        self.dim_notcond = dim_notcond\n",
    "        self.dim_cond = dim_cond\n",
    "        self.net_notcond = network(int(self.dim_notcond / 2), int(self.dim_notcond), *network_args)\n",
    "        self.net_cond = network(self.dim_cond, int(self.dim_notcond), *network_args)\n",
    "\n",
    "    def forward(self, x, x_condition):\n",
    "        \"\"\"\n",
    "        Forward pass of the affine coupling layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            x_condition (torch.Tensor): Condition tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying the affine coupling layer.\n",
    "            torch.Tensor: Log determinant of the Jacobian.\n",
    "        \"\"\"\n",
    "        x.float()\n",
    "        x_condition.float()\n",
    "        x_a, x_b = x.chunk(2, dim=1)\n",
    "        log_s, t = (self.net_notcond(x_b) * self.net_cond(x_condition)).chunk(2, dim=1)\n",
    "        # s = torch.exp(log_s)\n",
    "        s = F.sigmoid(log_s)\n",
    "        y_a = s * x_a + t\n",
    "        y_b = x_b\n",
    "\n",
    "        logdet = torch.sum(torch.log(s))\n",
    "\n",
    "        return torch.cat([y_a, y_b], dim=1), logdet\n",
    "\n",
    "    def backward(self, y, x_condition):\n",
    "        \"\"\"\n",
    "        Backward pass of the affine coupling layer.\n",
    "\n",
    "        Args:\n",
    "            y (torch.Tensor): Input tensor.\n",
    "            x_condition (torch.Tensor): Condition tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "            torch.Tensor: Log determinant of the Jacobian.\n",
    "        \"\"\"\n",
    "        y.float()\n",
    "        x_condition.float()\n",
    "        y_a, y_b = y.chunk(2, dim=1)\n",
    "        log_s, t = (self.net_notcond(y_b) * self.net_cond(x_condition)).chunk(2, dim=1)\n",
    "        # s = torch.exp(log_s)\n",
    "        s = F.sigmoid(log_s)\n",
    "        x_a = (y_a - t) / s\n",
    "        x_b = y_b\n",
    "\n",
    "        logdet = torch.sum(torch.log(s))\n",
    "\n",
    "        return torch.cat([x_a, x_b], dim=1), logdet\n",
    "\n",
    "class NF_condGLOW(nn.Module):\n",
    "    \"\"\"Normalizing flow GLOW model with Affine coupling layers. Alternates coupling layers with GLOW convolutions Combines coupling layers and convolution layers.\"\"\"\n",
    "\n",
    "    def __init__(self, n_layers, dim_notcond, dim_cond, CL=AffineCoupling, **kwargs_CL):\n",
    "        \"\"\"\n",
    "        Constructs a Normalizing flow model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        n_layers : int\n",
    "            The number of flow layers. Flow layers consist of a coupling layer and a convolution layer.\n",
    "        dim_notcond : int\n",
    "            The dimension of the input, i.e. the dimension of the data that will be transformed.\n",
    "        dim_cond : int\n",
    "            The dimension of the condition. If 0, the coupling layer is not conditioned.\n",
    "        CL : nn.Module\n",
    "            The coupling layer to use. Affine coupling layers is the only available for now\n",
    "        **kwargs_CL : dict\n",
    "            The arguments for the coupling layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_notcond = dim_notcond\n",
    "        self.dim_cond = dim_cond\n",
    "\n",
    "        coupling_layers = [CL(dim_notcond, dim_cond, **kwargs_CL) for _ in range(n_layers)]\n",
    "        conv_layers = [GLOW_conv(dim_notcond) for _ in range(n_layers)]\n",
    "\n",
    "\n",
    "        self.layers = nn.ModuleList(itertools.chain(*zip(conv_layers,coupling_layers)))\n",
    "        \n",
    "        self.prior = torch.distributions.MultivariateNormal(torch.zeros(2), torch.eye(2), validate_args=False)\n",
    "\n",
    "        #Information about hyperparameters accessible from outside\n",
    "        #The function _get_right_hypers below will then reconstruct this back to __init__ arguments, if you change the model, change both.\n",
    "        #This is needed in the background for recreating the same model in some cases like sampling with multiprocessing\n",
    "        kwargs_parsed = kwargs_CL.copy()\n",
    "        kwargs_parsed[\"network\"] = \"MLP\"\n",
    "        self.give_kwargs = {\"n_layers\":n_layers, \"dim_notcond\":dim_notcond, \"dim_cond\":dim_cond, \"CL\":\"AffineCoupling\", **kwargs_parsed}\n",
    "        \n",
    "    def forward(self, x, x_cond):\n",
    "        logdet = torch.zeros(x.shape[0]).to(x.device)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x, logdet_temp = layer.forward(x, x_cond)\n",
    "            logdet += logdet_temp\n",
    "            \n",
    "        #Get p_z(f(x)) which is needed for loss function together with logdet\n",
    "        prior_z_logprob = self.prior.log_prob(x).sum(-1)\n",
    "        \n",
    "        return x, logdet, prior_z_logprob\n",
    "    \n",
    "    def backward(self, y, x_cond):\n",
    "        logdet = torch.zeros(y.shape[0]).to(y.device)\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            y, logdet_temp = layer.backward(y, x_cond)\n",
    "            logdet += logdet_temp\n",
    "            \n",
    "        return y, logdet\n",
    "    \n",
    "    def sample_Flow(self, number, x_cond):\n",
    "        \"\"\"Samples from the prior and transforms the samples with the flow.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        number : int\n",
    "            The number of samples to draw. If a condition is given, the number of samples must be the same as the length of conditions.\n",
    "        x_cond : torch.Tensor\n",
    "            The condition for the samples. If dim_cond=0 enter torch.Tensor([]).\n",
    "        \"\"\"\n",
    "        return self.backward( self.prior.sample(torch.Size((number,))), torch.from_numpy(x_cond).to(device) )[0]\n",
    "    \n",
    "    def to(self, device):\n",
    "        #Modified to also move the prior to the right device\n",
    "        super().to(device)\n",
    "        self.prior = torch.distributions.Normal(torch.zeros(self.dim_notcond).to(device), torch.ones(self.dim_notcond).to(device))\n",
    "        return self\n",
    "    \n",
    "def training_flow(flow:NF_condGLOW, data:pd.DataFrame, cond_names:list,  epochs, lr=2*10**-2, batch_size=1024, loss_saver=None, checkpoint_dir=None, gamma=0.998, optimizer_obj=None):\n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    #Device the model is on\n",
    "    device = flow.parameters().__next__().device\n",
    "\n",
    "    #Get index based masks for conditional variables\n",
    "    mask_cond = np.isin(data.columns.to_list(), cond_names)\n",
    "    mask_cond = torch.from_numpy(mask_cond).to(device)\n",
    "    \n",
    "\n",
    "    # Convert DataFrame to tensor (index based)\n",
    "    data = torch.from_numpy(data.values).type(torch.float)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    if optimizer_obj is None:\n",
    "        optimizer = optim.Adam(flow.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optimizer_obj\n",
    "\n",
    "    lr_schedule = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=gamma)\n",
    "\n",
    "    #Save losses\n",
    "    if loss_saver is None:\n",
    "        losses = []\n",
    "    else:\n",
    "        losses = loss_saver\n",
    "\n",
    "    #Total number of steps\n",
    "    ct = 0\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    best_loss = 1_000_000\n",
    "    for e in tqdm(range(epochs)):\n",
    "        running_loss = 0\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            x = batch.to(device)\n",
    "            \n",
    "            #Evaluate model\n",
    "            z, logdet, prior_z_logprob = flow(x[..., ~mask_cond], x[..., mask_cond])\n",
    "            \n",
    "            #Get loss\n",
    "            loss = -torch.mean(logdet+prior_z_logprob) \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            #Set gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            #Compute gradients\n",
    "            loss.backward()\n",
    "            #Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Gather data and report\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 10 == 9:\n",
    "                last_loss = running_loss / 10 # loss per batch\n",
    "                print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "                tb_x = e * len(data_loader) + i + 1\n",
    "                writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "                \n",
    "                if last_loss < best_loss:\n",
    "                    best_loss = last_loss\n",
    "                    torch.save(flow.state_dict(), f\"{checkpoint_dir}checkpoint_best.pth\")\n",
    "                    print(f'state dict saved checkpoint_best')\n",
    "                    curr_time = time.perf_counter()\n",
    "                    np.save(f\"{checkpoint_dir}losses_best.npy\", np.array(best_loss))\n",
    "                    \n",
    "                running_loss = 0.\n",
    "\n",
    "            ct += 1\n",
    "\n",
    "            #Decrease learning rate every 10 steps until it is smaller than 3*10**-6, then every 120 steps\n",
    "            if lr_schedule.get_last_lr()[0] <= 3*10**-6:\n",
    "                decrease_step = 120\n",
    "            else:\n",
    "                decrease_step = 10\n",
    "\n",
    "            #Update learning rate every decrease_step steps\n",
    "            if ct % decrease_step == 0:\n",
    "                lr_schedule.step()\n",
    "\n",
    "def training_flow_MixedPrecision(flow:NF_condGLOW, data:pd.DataFrame, cond_names:list,  epochs, lr=2*10**-2, batch_size=1024, loss_saver=None, checkpoint_dir=None, gamma=0.998, optimizer_obj=None):\n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    #Device the model is on\n",
    "    device = flow.parameters().__next__().device\n",
    "\n",
    "    #Get index based masks for conditional variables\n",
    "    mask_cond = np.isin(data.columns.to_list(), cond_names)\n",
    "    mask_cond = torch.from_numpy(mask_cond).to(device)\n",
    "    \n",
    "    # Convert DataFrame to tensor (index based)\n",
    "    data = torch.from_numpy(data.values).type(torch.float)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    if optimizer_obj is None:\n",
    "        optimizer = optim.Adam(flow.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optimizer_obj\n",
    "    \n",
    "    scaler = GradScaler()\n",
    "\n",
    "    lr_schedule = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=gamma)\n",
    "\n",
    "    #Save losses\n",
    "    if loss_saver is None:\n",
    "        losses = []\n",
    "    else:\n",
    "        losses = loss_saver\n",
    "\n",
    "    #Total number of steps\n",
    "    ct = 0\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    best_loss = 1_000_000\n",
    "    for e in tqdm(range(epochs)):\n",
    "        running_loss = 0\n",
    "        for i, batch in enumerate(data_loader):\n",
    "     \n",
    "            #Set gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                x = batch.to(device)\n",
    "                #Evaluate model\n",
    "                z, logdet, prior_z_logprob = flow(x[..., ~mask_cond], x[..., mask_cond])\n",
    "                #Get loss\n",
    "                loss = -torch.mean(logdet+prior_z_logprob) \n",
    "                losses.append(loss.item())\n",
    "                \n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            #Update parameters\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "             # Gather data and report\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 10 == 9:\n",
    "                last_loss = running_loss / 10 # loss per batch\n",
    "                print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "                tb_x = e * len(data_loader) + i + 1\n",
    "                writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "                \n",
    "                if last_loss < best_loss:\n",
    "                    best_loss = last_loss\n",
    "                    torch.save(flow.state_dict(), f\"{checkpoint_dir}checkpoint_best.pth\")\n",
    "                    print(f'state dict saved checkpoint_best')\n",
    "                    curr_time = time.perf_counter()\n",
    "                    np.save(f\"{checkpoint_dir}losses_best.npy\", np.array(best_loss))\n",
    "                    \n",
    "                running_loss = 0.\n",
    "        \n",
    "            ct += 1\n",
    "\n",
    "            #Decrease learning rate every 10 steps until it is smaller than 3*10**-6, then every 120 steps\n",
    "            if lr_schedule.get_last_lr()[0] <= 3*10**-6:\n",
    "                decrease_step = 120\n",
    "            else:\n",
    "                decrease_step = 10\n",
    "\n",
    "            #Update learning rate every decrease_step steps\n",
    "            if ct % decrease_step == 0:\n",
    "                lr_schedule.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_parquet('/export/home/vgiusepp/MW_MH/data/preprocessing/preprocess_training_set.parquet')\n",
    "cond_names = list(data.keys()[2:])\n",
    "\n",
    "device = torch.device(\"cuda:9\" if torch.cuda.is_available() else \"cpu\")\n",
    "Flow = NF_condGLOW(n_layers=3, dim_notcond=2, dim_cond=12).to(device=device)\n",
    "Flow.load_state_dict(torch.load('../architecture/checkpoints/checkpoint_data/checkpoint_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_ofe= np.percentile(np.load('../../data/preprocessing/OFe.npy'), 0.1)\n",
    "min_feh = np.percentile(np.load('../../data/preprocessing/FeH.npy'), 0.1)\n",
    "\n",
    "data_temp = data[(data['feh'] > min_feh) & (data['ofe'] > min_ofe)] \n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.hist2d(data_temp[(data_temp['star_mass']==float(data_temp.head(1)['star_mass']))].values[:, 0], data_temp[(data_temp['star_mass']==float(data_temp.head(1)['star_mass']))].values[:, 1], bins=50);\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.hist2d(data[(data['star_mass']==float(data.head(1)['star_mass']))].values[:, 0], data[(data['star_mass']==float(data.head(1)['star_mass']))].values[:, 1], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Double and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mFlow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_Flow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_temp\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_temp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstar_mass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_temp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstar_mass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 252\u001b[0m, in \u001b[0;36mNF_condGLOW.sample_Flow\u001b[0;34m(self, number, x_cond)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_Flow\u001b[39m(\u001b[38;5;28mself\u001b[39m, number, x_cond):\n\u001b[1;32m    242\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Samples from the prior and transforms the samples with the flow.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m        The condition for the samples. If dim_cond=0 enter torch.Tensor([]).\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprior\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_cond\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[1], line 236\u001b[0m, in \u001b[0;36mNF_condGLOW.backward\u001b[0;34m(self, y, x_cond)\u001b[0m\n\u001b[1;32m    233\u001b[0m logdet \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(y\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m--> 236\u001b[0m     y, logdet_temp \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m     logdet \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m logdet_temp\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y, logdet\n",
      "Cell \u001b[0;32mIn[1], line 170\u001b[0m, in \u001b[0;36mAffineCoupling.backward\u001b[0;34m(self, y, x_condition)\u001b[0m\n\u001b[1;32m    168\u001b[0m x_condition\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    169\u001b[0m y_a, y_b \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 170\u001b[0m log_s, t \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_notcond(y_b) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet_cond\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_condition\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# s = torch.exp(log_s)\u001b[39;00m\n\u001b[1;32m    172\u001b[0m s \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msigmoid(log_s)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 56\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     55\u001b[0m     x\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Double and Float"
     ]
    }
   ],
   "source": [
    "Flow.sample_Flow(1000, data_temp[(data_temp['star_mass']==float(data_temp.head(1)['star_mass']))].values[0, 2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_temp[(data_temp['star_mass']==float(data_temp.head(1)['star_mass']))].values[0, 2:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
